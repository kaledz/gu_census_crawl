{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c39acd96-cbbf-497f-ae6d-fa327d3e7987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Wet File Data Extraction**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c15fd1f0-5a1a-4a07-b6f9-829152f1a14e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85bee9fd-c83f-44e6-bffd-c896c4d2d1c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import boto3\n",
    "import botocore\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52b43ba1-7453-499a-8eda-83194ca68631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Connect to Boto3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94f77efa-cc51-46b5-98ba-9be74c96e12a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.1 Set Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71753b31-0137-4637-9b03-fd4591ac43b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "aws_access_key_id = dbutils.secrets.get(scope='aws_cc', key='aws_access_key_id')\n",
    "aws_secret_access_key = dbutils.secrets.get(scope='aws_cc', key='aws_secret_access_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6c9b7b0-70b1-44df-9b82-91854363519d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.2 Intialize boto3 client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90566ef5-b594-4c43-887e-d70e7dcb3560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optional: build client once (faster)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=\"us-east-1\",\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2553dbf9-7d5e-48b5-a817-b8b47d38d86c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Read Bronze Layer\n",
    "- Read in all crawls from bronze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "568d25d9-5318-42ad-855d-b19884d3abd0",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"LastModified\":137,\"StorageClass\":149,\"Key\":279,\"ETag\":186},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760901621332}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1757450377762}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Read the raw crawls table and filter for keys containing 'wet'\n",
    "df_all_crawls = (\n",
    "    spark.read.table(\"census_bureau_capstone.bronze.raw_all_crawls\")\n",
    "    .filter(F.col(\"key\").contains(\"wet\"))\n",
    ")\n",
    "\n",
    "# Extract crawl start and end timestamps from the 'key' column\n",
    "df_all_crawls = df_all_crawls.withColumn(\n",
    "    \"crawl_start_raw\",\n",
    "    F.regexp_extract(F.col(\"key\"), r\"CC-MAIN-(\\d{14})-(\\d{14})-\", 1)\n",
    ").withColumn(\n",
    "    \"crawl_end_raw\",\n",
    "    F.regexp_extract(F.col(\"key\"), r\"CC-MAIN-(\\d{14})-(\\d{14})-\", 2)\n",
    ")\n",
    "\n",
    "# Convert extracted timestamp strings to timestamp type\n",
    "df_all_crawls = df_all_crawls.withColumn(\n",
    "    \"crawl_start\",\n",
    "    F.expr(\"try_to_timestamp(crawl_start_raw, 'yyyyMMddHHmmss')\")\n",
    ").withColumn(\n",
    "    \"crawl_end\",\n",
    "    F.expr(\"try_to_timestamp(crawl_end_raw, 'yyyyMMddHHmmss')\")\n",
    ").orderBy(\"crawl_start\")\n",
    "\n",
    "# Filter out rows where crawl_start could not be parsed\n",
    "df_all_crawls = df_all_crawls.filter(F.col(\"crawl_start\").isNotNull())\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_all_crawls = df_all_crawls.drop(\"crawl_start_raw\", \"crawl_end_raw\", \"ChecksumAlgorithm\")\n",
    "\n",
    "# Count total number of rows\n",
    "total = df_all_crawls.count()\n",
    "\n",
    "print(f\"total: {total:,}\")\n",
    "display(df_all_crawls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20d68959-3183-4f38-be84-2fc66fa76538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.1 Sort crawls\n",
    "- random sample from past 5 years for each\n",
    "- limit to 1 sample for each year\n",
    "- set to python list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60308570-1511-4462-abeb-bbc54d316dec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3.1.1 Random Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd0ff95e-4ea2-46ed-a38e-f58a475d9da1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"ChecksumAlgorithm\":90,\"crawl_prefix\":318,\"Key\":754},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760899644549}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "years = ['2021', '2022', '2023', '2024', '2025']\n",
    "samples = []\n",
    "\n",
    "def sample_crawls(years, samples, df):\n",
    "    for year in years:\n",
    "        # Filter the DataFrame for each year and 'wet' keyword, then take a single random sample\n",
    "        sample = df.limit(2)\n",
    "        \n",
    "        # Check if the sample is not empty before appending\n",
    "        if sample.count() > 0:\n",
    "            samples.append(sample)\n",
    "\n",
    "    # Initialize the combined DataFrame with the first sample if samples list is not empty\n",
    "    if samples:\n",
    "        df = samples[0]\n",
    "\n",
    "        # Union the rest of the samples into the combined DataFrame\n",
    "        for sample in samples[1:]:\n",
    "            df = df.union(sample)\n",
    "\n",
    "        # Convert to pandas\n",
    "        df = df.toPandas()\n",
    "        return df\n",
    "\n",
    "    else:\n",
    "        print(\"No samples found for the given criteria.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a20ff338-1648-4525-9e58-e6e7063576ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3.1.2 Set to python list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49fba408-3508-4a65-a2bb-a1a470dc2a38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "key_list = df_all_crawls_samples['Key'].tolist()\n",
    "\n",
    "print(key_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a252a2c7-8a77-424f-98c9-ed44b288b376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1300efb4-34d9-414f-879b-5cb840587ca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.1 Extract CC Wet file\n",
    "- Extracrts the random sample text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b44bfa2c-44f5-4e6a-ac26-219ec8cc9daa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_bucket = \"commoncrawl\"\n",
    "destination_bucket = 'mydbxbucketpractice'\n",
    "\n",
    "\n",
    "def download_and_upload(source_key, destination_key):\n",
    "    for source_key in key_list:\n",
    "        destination_key = (\n",
    "            'common_crawl/wet_files/' +\n",
    "            source_key.split(\"/\")[-1]\n",
    "        )\n",
    "        local_filename = '/tmp/' + source_key.split(\"/\")[-1]\n",
    "        \n",
    "        s3.download_file(source_bucket, source_key, local_filename)\n",
    "        s3.upload_file(local_filename, destination_bucket, destination_key)\n",
    "        os.remove(local_filename)\n",
    "        print(\n",
    "            f\"Copied s3://{source_bucket}/{source_key} to \"\n",
    "            f\"s3://{destination_bucket}/{destination_key}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "587cf290-32a0-49da-840a-c3cbd99eb6f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.2. View raw file as df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f38e38-dfc7-410b-9a5a-4688ee7fd7aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_paths = [f\"s3://{destination_bucket}/common_crawl/wet_files/{file.name}\" for file in dbutils.fs.ls(f\"s3://{destination_bucket}/common_crawl/wet_files/\")]\n",
    "\n",
    "\n",
    "df = spark.read.text(file_paths)\n",
    "display(df)\n",
    "df = df\n",
    "\n",
    "total = df.count()\n",
    "\n",
    "print(f\"total: {total:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "180a9db1-c7ef-448c-a090-559bf8668c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4.2.1 Drop Files if needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb927836-4953-418a-be93-96922f066812",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop all files from the directory\n",
    "# for file_path in file_paths:\n",
    "   # dbutils.fs.rm(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf019db5-cdea-4a3f-b5d4-9de74194a985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.3. Text transformation"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "wet_files_extraction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
