{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07fdd80d-debf-4dcb-8815-cc5ef8c468e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Helper Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "195018d1-e36b-488d-b749-c2ac3299f778",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ac657c-16a0-4455-b7b9-0954d722e2c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1: S3 Data ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea4d1d93-2baf-41c4-a775-26605aaf9cdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "from functools import reduce\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "class cc():\n",
    "    def list_master_indexes():\n",
    "        # List all master indexes from the Common Crawl S3 bucket\n",
    "        bucket = \"commoncrawl\"\n",
    "        prefix = \"crawl-data/\"\n",
    "        paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "        result = []\n",
    "        for page in paginator.paginate(\n",
    "            Bucket=bucket,\n",
    "            Prefix=prefix,\n",
    "            Delimiter=\"/\"\n",
    "        ):\n",
    "            result.extend(page.get(\"CommonPrefixes\", []))\n",
    "        return pd.DataFrame([p['Prefix'] for p in result], columns=[\"master_index\"])\n",
    "    \n",
    "    def list_crawls(prefix: str, as_json: bool = False, s3_client=None) -> pd.DataFrame | str:\n",
    "        \"\"\"\n",
    "        List S3 objects under the given Common Crawl prefix and return as pandas DataFrame (or JSON).\n",
    "        Example prefix: 'crawl-data/CC-MAIN-2025-05/'  (trailing slash recommended)\n",
    "        \"\"\"\n",
    "        bucket_name = \"commoncrawl\"\n",
    "        s3c = s3_client or s3\n",
    "\n",
    "        # normalize prefix\n",
    "        if not prefix or not isinstance(prefix, str):\n",
    "            raise ValueError(f\"prefix must be a non-empty str, got: {type(prefix).__name__}={prefix!r}\")\n",
    "        if not prefix.endswith(\"/\"):\n",
    "            prefix = prefix + \"/\"\n",
    "\n",
    "        object_metadata = []\n",
    "        paginator = s3c.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "        for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "            # some pages may not have 'Contents' (e.g., empty prefixes)\n",
    "            contents = page.get(\"Contents\", [])\n",
    "            if contents:\n",
    "                object_metadata.extend(contents)\n",
    "\n",
    "        if not object_metadata:\n",
    "            # empty result\n",
    "            if as_json:\n",
    "                return json.dumps([])\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(object_metadata)\n",
    "        if as_json:\n",
    "            return df.to_json(orient=\"records\")\n",
    "        return df\n",
    "        \n",
    "    def batch_crawl_list(df, column):\n",
    "        # Prepare a list of crawl prefixes from the given DataFrame column\n",
    "        crawl_list = (\n",
    "            df[column]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .apply(lambda s: s if s.endswith(\"/\") else s + \"/\")\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        # de-dupe while preserving order\n",
    "        seen = set()\n",
    "        crawl_list = [p for p in crawl_list if p and not (p in seen or seen.add(p))]\n",
    "        return crawl_list\n",
    "    \n",
    "    def batch_ingest_crawls(crawl_list):\n",
    "        \"\"\"\n",
    "        Runs list_crawls(prefix) for each prefix and unions into a single Spark DataFrame.\n",
    "        Assumes list_crawls returns a pandas DataFrame.\n",
    "        \"\"\"\n",
    "        spark_parts = []\n",
    "        total = len(crawl_list)\n",
    "\n",
    "        for i, pfx in enumerate(crawl_list, 1):\n",
    "            print(f\"[{i}/{total}] Fetching: {pfx}\")\n",
    "            pdf = cc.list_crawls(pfx, as_json=False)  # <-- just call directly\n",
    "            if pdf is None or pdf.empty:\n",
    "                print(f\"  -> empty; skipping {pfx}\")\n",
    "                continue\n",
    "\n",
    "            sdf = spark.createDataFrame(pdf).withColumn(\"crawl_prefix\", F.lit(pfx))\n",
    "            spark_parts.append(sdf)\n",
    "\n",
    "        if not spark_parts:\n",
    "            raise RuntimeError(\"No data returned to union.\")\n",
    "\n",
    "        # Union all Spark DataFrames into a single DataFrame\n",
    "        df_crawls = reduce(lambda a, b: a.unionByName(b, allowMissingColumns=True), spark_parts)\n",
    "        print(f\"Total rows: {df_crawls.count()} | Prefixes combined: {len(spark_parts)}\")\n",
    "        return df_crawls"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "helper_functions_new",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
