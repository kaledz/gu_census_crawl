{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07fdd80d-debf-4dcb-8815-cc5ef8c468e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Helper Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "195018d1-e36b-488d-b749-c2ac3299f778",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79ac657c-16a0-4455-b7b9-0954d722e2c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1: S3 Data ingestion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "628a3a70-a1e4-4b80-9fb2-1c10d2b3e56e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.1: List the master indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea4d1d93-2baf-41c4-a775-26605aaf9cdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def list_master_indexes():\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        region_name=\"us-east-1\",\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key\n",
    "    )\n",
    "    bucket = \"commoncrawl\"\n",
    "    prefix = \"crawl-data/\"\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    result = []\n",
    "    for page in paginator.paginate(\n",
    "        Bucket=bucket,\n",
    "        Prefix=prefix,\n",
    "        Delimiter=\"/\"\n",
    "    ):\n",
    "        result.extend(page.get(\"CommonPrefixes\", []))\n",
    "    return pd.DataFrame([p['Prefix'] for p in result], columns=[\"master_index\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38c2f34d-6418-4546-a3bb-1ca2d23346e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.2 List all crawls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4a56247-ce85-4f64-9de1-f94215561393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Optional: build client once (faster)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=\"us-east-1\",\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    ")\n",
    "\n",
    "def list_crawls(prefix: str, as_json: bool = False, s3_client=None) -> pd.DataFrame | str:\n",
    "    \"\"\"\n",
    "    List S3 objects under the given Common Crawl prefix and return as pandas DataFrame (or JSON).\n",
    "    Example prefix: 'crawl-data/CC-MAIN-2025-05/'  (trailing slash recommended)\n",
    "    \"\"\"\n",
    "    bucket_name = \"commoncrawl\"\n",
    "    s3c = s3_client or s3\n",
    "\n",
    "    # normalize prefix\n",
    "    if not prefix or not isinstance(prefix, str):\n",
    "        raise ValueError(f\"prefix must be a non-empty str, got: {type(prefix).__name__}={prefix!r}\")\n",
    "    if not prefix.endswith(\"/\"):\n",
    "        prefix = prefix + \"/\"\n",
    "\n",
    "    object_metadata = []\n",
    "    paginator = s3c.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "        # some pages may not have 'Contents' (e.g., empty prefixes)\n",
    "        contents = page.get(\"Contents\", [])\n",
    "        if contents:\n",
    "            object_metadata.extend(contents)\n",
    "\n",
    "    if not object_metadata:\n",
    "        # empty result\n",
    "        if as_json:\n",
    "            return json.dumps([])\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(object_metadata)\n",
    "    if as_json:\n",
    "        return df.to_json(orient=\"records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "793f7ba3-6576-46ea-a460-f88d91b6f603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.3 Create Batch Crawl List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "560ab350-9eaa-4da8-96d6-07a47fa42db7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def batch_crawl_list(df, column):\n",
    "    crawl_list = (\n",
    "        df[column]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .apply(lambda s: s if s.endswith(\"/\") else s + \"/\")\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    # de-dupe while preserving order\n",
    "    seen = set()\n",
    "    crawl_list = [p for p in crawl_list if p and not (p in seen or seen.add(p))]\n",
    "    return crawl_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06642634-a3d1-4b4a-9fc8-9f9b2e021fbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.4 Execute Batch ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f853f80-67bd-4855-ad89-af5c561c4a4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def batch_ingest_crawls(crawl_list):\n",
    "    \"\"\"\n",
    "    Runs list_crawls(prefix) for each prefix and unions into a single Spark DataFrame.\n",
    "    Assumes list_crawls returns a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    spark_parts = []\n",
    "    total = len(crawl_list)\n",
    "\n",
    "    for i, pfx in enumerate(crawl_list, 1):\n",
    "        print(f\"[{i}/{total}] Fetching: {pfx}\")\n",
    "        pdf = list_crawls(pfx, as_json=False)  # <-- just call directly\n",
    "        if pdf is None or pdf.empty:\n",
    "            print(f\"  -> empty; skipping {pfx}\")\n",
    "            continue\n",
    "\n",
    "        sdf = spark.createDataFrame(pdf).withColumn(\"crawl_prefix\", F.lit(pfx))\n",
    "        spark_parts.append(sdf)\n",
    "\n",
    "    if not spark_parts:\n",
    "        raise RuntimeError(\"No data returned to union.\")\n",
    "\n",
    "    df_crawls = reduce(lambda a, b: a.unionByName(b, allowMissingColumns=True), spark_parts)\n",
    "    print(f\"Total rows: {df_crawls.count()} | Prefixes combined: {len(spark_parts)}\")\n",
    "    return df_crawls\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "helper_functions_new",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
