{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a041fe5-1950-48a4-abf3-973ff76a7c7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c870ddb7-150b-4a57-b4be-91929131f14c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%pip install beautifulsoup4\n",
    "\n",
    "\n",
    "# import the necessary packages\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.error import URLError, HTTPError\n",
    "from urllib.parse import urljoin, quote\n",
    "import requests\n",
    "import os\n",
    "import gzip\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "#--------------------------------Class and functions for parsing web data-----------------------------\n",
    "# Define headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "class WebParser():\n",
    "    # Function to fetch HTML content\n",
    "    def fetch_html(url):\n",
    "        try:\n",
    "            req = urllib.request.Request(url, headers=headers)\n",
    "            with urllib.request.urlopen(req) as response:\n",
    "                content_type = response.getheader('Content-Type')\n",
    "                content = response.read()\n",
    "\n",
    "                # Check if the content type is available\n",
    "                if content_type:\n",
    "                    # Split the content type if it includes a comma\n",
    "                    parts = content_type.split(\",\", 1)\n",
    "                    mediatype = parts[0]  # Get the primary media type\n",
    "\n",
    "                    # Check if the content is text/html and decode accordingly\n",
    "                    if 'text/html' in mediatype:\n",
    "                        try:\n",
    "                            return content.decode('utf-8')\n",
    "                        except UnicodeDecodeError:\n",
    "                            print(f\"Decoding error for {url}, returning raw content\")\n",
    "                            return content  # Return raw binary if decoding fails\n",
    "                else:\n",
    "                    print(f\"No content type received for {url}, returning raw content\")\n",
    "                    return content  # Return raw content if no content type\n",
    "\n",
    "        except (HTTPError, URLError) as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            return ''\n",
    "\n",
    "    # Function to parse HTML and find CSS and JavaScript links\n",
    "    def get_assets(html, base_url):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Find all CSS links\n",
    "        css_links = []\n",
    "        for link in soup.find_all('link', rel='stylesheet'):\n",
    "            href = link.get('href')\n",
    "            if href:\n",
    "                full_url = urljoin(base_url, href)\n",
    "                # Encode URL to handle spaces and other special characters\n",
    "                css_links.append(quote(full_url, safe='/:?&='))\n",
    "\n",
    "        # Find all JavaScript links\n",
    "        js_links = []\n",
    "        for script in soup.find_all('script', src=True):\n",
    "            src = script.get('src')\n",
    "            if src:\n",
    "                full_url = urljoin(base_url, src)\n",
    "                # Encode URL to handle spaces and other special characters\n",
    "                js_links.append(quote(full_url, safe='/:?&='))\n",
    "\n",
    "        return css_links, js_links\n",
    "\n",
    "    # Function to download resources with handling for binary content\n",
    "    def download_resource(url):\n",
    "        try:\n",
    "            req = urllib.request.Request(url, headers=headers)\n",
    "            with urllib.request.urlopen(req) as response:\n",
    "                content = response.read()\n",
    "\n",
    "                # Try to decode as text, fallback to binary\n",
    "                try:\n",
    "                    return content.decode('utf-8')\n",
    "                except:\n",
    "                    return content  # Return raw binary if decoding fails\n",
    "        except:\n",
    "            # print(f\"Error downloading {url}: {e}\")\n",
    "            return ''\n",
    "#----------------------------------------Class for data extraction functions of common crawl data--------------------------        \n",
    "class CrawlExtractor():\n",
    "    #Function to get WET file paths for US-specific URLs from Common Crawl Index\n",
    "    def get_us_wet_file_urls(query_url):\n",
    "        response = requests.get(query_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            wet_file_urls = set()\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    record = json.loads(line.decode('utf-8'))\n",
    "                    url = record.get('url', '')\n",
    "                    filename = record.get('filename', '')\n",
    "\n",
    "                    if filename.endswith('.warc.gz'):\n",
    "                        wet_filename = filename.replace('.warc.gz', '.warc.wet.gz').replace(\"/warc/\", \"/wet/\")\n",
    "                        wet_file_urls.add(f\"https://data.commoncrawl.org/{wet_filename}\")\n",
    "            return list(wet_file_urls)\n",
    "        else:\n",
    "            print(f\"Failed to fetch data: {response.status_code}\")\n",
    "            return []\n",
    "        \n",
    "    # Function to download a file from a URL\n",
    "    def download_file(url, dest_folder):\n",
    "        if not os.path.exists(dest_folder):\n",
    "            os.makedirs(dest_folder)\n",
    "        local_filename = os.path.join(dest_folder, url.split('/')[-1])\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(local_filename, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "        return local_filename\n",
    "\n",
    "    # Function to extract text from WET file\n",
    "    def extract_text_from_wet(wet_file_path):\n",
    "        with gzip.open(wet_file_path, 'rb') as stream:\n",
    "            for record in ArchiveIterator(stream):\n",
    "                if record.rec_type == 'conversion':\n",
    "                    text = record.content_stream().read().decode('utf-8')\n",
    "                    yield record.rec_headers.get_header('WARC-Target-URI'), text\n",
    "\n",
    "    # Function to check for US Census Bureau mentions\n",
    "    def contains_us_census_info(content):\n",
    "        keywords = [\"US Census Bureau\", \"USCB\", \"Census Bureau\"]\n",
    "        return any(re.search(keyword, content, re.IGNORECASE) for keyword in keywords)\n",
    "\n",
    "    # Function to save URLs\n",
    "    def save_urls(urls, output_file):\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(urls, f, indent=4)\n",
    "\n",
    "    def extract_urls_from_warc(warc_wet_gz_file):\n",
    "        file = download_file(warc_wet_gz_file, \"wet_files\")\n",
    "        urls = []\n",
    "        with gzip.open(file, 'rb') as stream:\n",
    "            for record in ArchiveIterator(stream):\n",
    "                if record.rec_type == 'conversion':  # WET files usually have 'conversion' records\n",
    "                    url = record.rec_headers.get_header('WARC-Target-URI')\n",
    "                    if url:\n",
    "                        urls.append(url)\n",
    "        os.remove(file)\n",
    "        return urls\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "polars"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "helper_functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
