{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "abcec557-34cf-4365-b042-2f3dfc9df24f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./(Clone) wet_files_extraction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c6154e-76e1-4e10-bfae-68dab2fc5bab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install thefuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d005a7b1-f79a-46ac-89fd-7b093bfa6a02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import lower\n",
    "from pyspark.ml import Pipeline\n",
    "from thefuzz import fuzz, process\n",
    "from typing import Iterator\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, NGram, CountVectorizer, IDF, VectorAssembler, StringIndexer, HashingTF\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC, RandomForestClassifier, OneVsRest\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import mlflow, mlflow.spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "888ffbc4-24ea-42fd-82b6-e9ad951ea06e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#runnning time ~2 mins\n",
    "df = warc_en.select('target_uri','body','host','base_domain','lang')\n",
    "display(df.limit(50))\n",
    "print('English docs:', df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c39bfb1-7982-414c-ad0a-bd862703c294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Fuzzy Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a98887e-9080-4e81-83b2-5362e8fc58f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setting body of text into lower case\n",
    "\n",
    "df = (\n",
    "    df.withColumn('body', F.lower(F.col('body')))\n",
    ")\n",
    "\n",
    "# Dictionary for Census and ACS general terminology for Fuzzy Matching \n",
    "\n",
    "GATES = {\n",
    "    # Mention ACS acronym or any two words from the phrase (more forgiving)\n",
    "    'ACS (American Community Survey)': re.compile(\n",
    "        r'\\b(acs|american\\s+community|community\\s+survey|american\\s+survey)\\b', re.I\n",
    "    ),\n",
    "\n",
    "    # Allow \"census\" alone OR \"census ... bureau\" (some pages omit \"bureau\")\n",
    "    'U.S. Census Bureau': re.compile(\n",
    "        r'(\\bcensus\\b.*\\b(bureau|bea(?:u|ue|eau)r(?:o|ou|eau)|buro)\\b|\\bcensus\\b)', re.I\n",
    "    ),\n",
    "\n",
    "    # Catch variants like \"1 yr\", \"one-year\", \"1-year\", \"1 year\"\n",
    "    'ACS (1-year)': re.compile(\n",
    "        r'\\bacs\\b.*\\b(1|one)\\s*[- ]?\\s*year\\b', re.I\n",
    "    ),\n",
    "\n",
    "    'ACS (5-year)': re.compile(\n",
    "        r'\\bacs\\b.*\\b(5|five)\\s*[- ]?\\s*year\\b', re.I\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Targets dictionary\n",
    "targets = {\n",
    "    'american community survey': 'ACS (American Community Survey)',\n",
    "    'u.s. census bureau':        'U.S. Census Bureau',\n",
    "    'acs 1-year':                'ACS (1-year)',\n",
    "    'acs 5-year':                'ACS (5-year)',\n",
    "}\n",
    "\n",
    "# Anchors (broader, to ensure we keep candidate coverage)\n",
    "anchors = r'(american|community|survey|\\bacs\\b|census|bureau|bea(?:u|ue|eau)r(?:o|ou|eau)|buro)'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "542ba114-77b4-4af1-b353-af92bca788d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# normilizing text\n",
    "\n",
    "def normalize(col='body'):\n",
    "    return F.trim(F.regexp_replace(F.lower(F.col(col)), r'\\s+', ' '))\n",
    "\n",
    "docs = (\n",
    "    df.withColumn('text_norm', normalize('body'))\n",
    "      .filter(F.length('text_norm') > 50)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a73099ad-bb16-4c6b-84a7-5edc40351794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = RegexTokenizer(\n",
    "    inputCol='text_norm',\n",
    "    outputCol='tokens',\n",
    "    pattern=r'[^a-z0-9]+',\n",
    "    gaps=True\n",
    ")\n",
    "tok_raw = tokenizer.transform(docs)  # docs is your filtered/normalized DF\n",
    "\n",
    "# 2) Add NGram columns (this is what creates ng1..ng4)\n",
    "for n in [1, 2, 3, 4]:\n",
    "    tok_raw = NGram(n=n, inputCol='tokens', outputCol=f'ng{n}').transform(tok_raw)\n",
    "\n",
    "empty_arr = F.lit([]).cast(T.ArrayType(T.StringType()))\n",
    "for n in [1, 2, 3, 4]:\n",
    "    if f'ng{n}' not in tok_raw.columns:\n",
    "        # If for any reason NGram didnâ€™t create a column, create an empty one\n",
    "        tok_raw = tok_raw.withColumn(f'ng{n}', empty_arr)\n",
    "    else:\n",
    "        tok_raw = tok_raw.withColumn(f'ng{n}', F.coalesce(F.col(f'ng{n}'), empty_arr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7db65457-e72d-4797-85a5-d4fa603057cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run time approximately ~8 mins\n",
    "\n",
    "# Merge & explode all n-grams into candidate phrases\n",
    "tok_exploded = (\n",
    "    tok_raw\n",
    "    .withColumn('all_ngrams', F.array_union(F.col('ng1'), F.col('ng2')))\n",
    "    .withColumn('all_ngrams', F.array_union(F.col('all_ngrams'), F.col('ng3')))\n",
    "    .withColumn('all_ngrams', F.array_union(F.col('all_ngrams'), F.col('ng4')))\n",
    "    .withColumn('cand', F.explode('all_ngrams'))\n",
    "    .select('target_uri', 'base_domain', 'text_norm', 'cand')\n",
    ")\n",
    "\n",
    "print('tok_exploded.count() =', tok_exploded.count())\n",
    "print('distinct pages in tok_exploded =', tok_exploded.select('target_uri').distinct().count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22bf707a-8e8d-46c7-958a-711054eeeb72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tok = tok_exploded.filter(F.col('cand').rlike(anchors))\n",
    "\n",
    "# Reduce per-page explosion (keep up to 3k distinct candidates/page)\n",
    "tok = (tok\n",
    "    .withColumn('tok_cnt', F.size(F.split(F.col('cand'), r'\\s+')))\n",
    "    .filter(F.col('tok_cnt') >= 2)\n",
    "    .filter(F.length('cand') >= 5)\n",
    "    .drop('tok_cnt')\n",
    ")\n",
    "\n",
    "# Ensure target_uri exists and is non-null before window\n",
    "if 'target_uri' not in tok.columns:\n",
    "    tok = tok.withColumn('target_uri', F.concat_ws('-', F.lit('row'), F.monotonically_increasing_id()))\n",
    "else:\n",
    "    tok = tok.filter(F.col('target_uri').isNotNull())\n",
    "\n",
    "\n",
    "w = Window.partitionBy('target_uri').orderBy(F.length('cand'))\n",
    "tok = (\n",
    "    tok.withColumn('rn', F.row_number().over(w))\n",
    "       .filter(F.col('rn') <= 3000)\n",
    "       .drop('rn')\n",
    ")\n",
    "\n",
    "# Collect candidates per page\n",
    "cand_by_page = (\n",
    "    tok.groupBy('target_uri','base_domain')\n",
    "       .agg(F.collect_set('cand').alias('cand_list'))\n",
    "       .join(docs.select('target_uri','text_norm'), on='target_uri', how='left')\n",
    ")\n",
    "\n",
    "cand_by_page = cand_by_page.withColumn(\n",
    "    'cand_list',\n",
    "    F.coalesce(\n",
    "        F.col('cand_list'),\n",
    "        F.lit([]).cast(T.ArrayType(T.StringType()))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Cleaning it up: trim, drop null/empty, de-dupe (optional but helpful)\n",
    "cand_by_page = cand_by_page.withColumn(\n",
    "    'cand_list',\n",
    "    F.array_distinct(\n",
    "        F.expr(\"filter(transform(cand_list, x -> trim(x)), x -> x is not null and x <> '')\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6108e6c-25b5-4b5c-b576-b80904e26f84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Disable all gates for one run\n",
    "GATES = {v: None for v in targets.values()}\n",
    "# re-run score_page_batch + scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e663ab-a70f-45c8-b6cc-09c486e9116e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = T.StructType([\n",
    "    T.StructField('target_uri',   T.StringType(), False),\n",
    "    T.StructField('base_domain',  T.StringType(), True),\n",
    "    T.StructField('target_label', T.StringType(), False),\n",
    "    T.StructField('target_text',  T.StringType(), False),\n",
    "    T.StructField('best_phrase',  T.StringType(), True),\n",
    "    T.StructField('best_score',   T.IntegerType(), True),\n",
    "])\n",
    "\n",
    "targets_pdf = pd.DataFrame([(k, v) for k, v in targets.items()],\n",
    "                           columns=['target_text', 'target_label'])\n",
    "\n",
    "def score_page_batch(rows: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "    for df_batch in rows:\n",
    "        out_rows = []\n",
    "        for _, r in df_batch.iterrows():\n",
    "            cands = r['cand_list'] if isinstance(r.get('cand_list', None), list) else []\n",
    "            cands = [c.strip() for c in cands if isinstance(c, str) and c.strip()]\n",
    "\n",
    "            for _, t in targets_pdf.iterrows():\n",
    "                label = t['target_label']\n",
    "                gate = GATES.get(label)\n",
    "\n",
    "                # Gate\n",
    "                gated_cands = [c for c in cands if gate.search(c)] if (gate and cands) else cands\n",
    "\n",
    "                # ðŸ” Fallback: if gate filtered away everything, use all candidates\n",
    "                if not gated_cands and cands:\n",
    "                    gated_cands = cands\n",
    "\n",
    "                if gated_cands:\n",
    "                    # lenient to find signal; switch to token_sort_ratio later if needed\n",
    "                    match = process.extractOne(t['target_text'], gated_cands, scorer=fuzz.token_set_ratio)\n",
    "                    best_phrase, best_score = (match[0], int(match[1])) if match else (None, None)\n",
    "                else:\n",
    "                    best_phrase, best_score = None, None\n",
    "\n",
    "                out_rows.append({\n",
    "                    'target_uri':   r['target_uri'],\n",
    "                    'base_domain':  r.get('base_domain'),\n",
    "                    'target_label': label,\n",
    "                    'target_text':  t['target_text'],\n",
    "                    'best_phrase':  best_phrase,\n",
    "                    'best_score':   best_score,\n",
    "                })\n",
    "        yield pd.DataFrame(out_rows)\n",
    "\n",
    "scored = cand_by_page.mapInPandas(score_page_batch, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c156d78b-bf8b-4565-817b-d383d6e5eb52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print('rows with non-null best_score:',\n",
    "      scored.filter(F.col('best_score').isNotNull()).count())\n",
    "\n",
    "display(\n",
    "    scored.orderBy(F.desc('best_score'))\n",
    "          .select('target_label','best_score','best_phrase','base_domain','target_uri')\n",
    "          .limit(30)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d923db8-020c-4ad4-b7c3-68e59aaf51d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# running time approximately ~8 mins\n",
    "\n",
    "thresh = 80\n",
    "scored_flagged = scored.withColumn('is_match', F.col('best_score') >= F.lit(thresh))\n",
    "\n",
    "print('Total scored rows:', scored.count())\n",
    "print('Matches above threshold:', scored_flagged.filter('is_match').count())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e98bad62-3bd6-4846-aae5-3061e186038a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    scored\n",
    "      .select('target_label','best_score','best_phrase','base_domain','target_uri')\n",
    "      .where(F.col('best_score').isNotNull())\n",
    "      .orderBy(F.desc('best_score'))\n",
    "      .limit(50)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "687d2d0b-d0b5-4f2d-b9f2-d4c4f4dbd8cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"tok columns:\", tok.columns)\n",
    "tok.select('target_uri').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cad51aa5-3fb0-4619-bf82-b7c0ce94e2a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# See the top matches to sanity-check quality\n",
    "display(\n",
    "    scored_flagged\n",
    "      .filter('is_match')\n",
    "      .select('target_label','best_score','best_phrase','base_domain','target_uri')\n",
    "      .orderBy(F.desc('best_score'))\n",
    "      .limit(50)\n",
    ")\n",
    "\n",
    "# Score distribution to decide final threshold\n",
    "score_dist = (\n",
    "    scored.filter(F.col('best_score').isNotNull())\n",
    "          .groupBy('target_label')\n",
    "          .agg(\n",
    "              F.expr('percentile_approx(best_score, 0.5)').alias('p50'),\n",
    "              F.expr('percentile_approx(best_score, 0.9)').alias('p90'),\n",
    "              F.max('best_score').alias('max_score'),\n",
    "              F.count('*').alias('pages_scored')\n",
    "          )\n",
    ")\n",
    "display(score_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d52bf3e-8d6b-4497-9a47-32eabb2825af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Any target matched on each page?\n",
    "page_any = (\n",
    "    scored_flagged.groupBy('target_uri')\n",
    "    .agg(\n",
    "        F.max(F.col('is_match').cast('int')).alias('any_match')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Sanity check counts\n",
    "print('Total scored rows:', scored.count())\n",
    "print('Matches above threshold:', scored_flagged.filter('is_match').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b959f035-e366-46e3-9949-142d1415919b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count distinct websites with ACS/Census matches\n",
    "domain_counts = (\n",
    "    scored_flagged\n",
    "    .filter(F.col('is_match') == True)\n",
    "    .select('base_domain')\n",
    "    .distinct()\n",
    "    .groupBy()\n",
    "    .agg(\n",
    "        F.count('*').alias('num_websites_with_terminology')\n",
    "    )\n",
    ")\n",
    "display(domain_counts)\n",
    "\n",
    "# Per-target website counts\n",
    "per_target_domains = (\n",
    "    scored_flagged\n",
    "    .filter((F.col('is_match') == True) & F.col('base_domain').isNotNull())\n",
    "    .groupBy('target_label')\n",
    "    .agg(F.approx_count_distinct('base_domain', 0.02).alias('distinct_websites'))\n",
    "    .orderBy(F.desc('distinct_websites'))\n",
    ")\n",
    "display(per_target_domains)\n",
    "\n",
    "# Score distribution for threshold tuning\n",
    "score_dist = (\n",
    "    scored_flagged\n",
    "    .filter(F.col('best_score').isNotNull())\n",
    "    .groupBy('target_label')\n",
    "    .agg(\n",
    "        F.expr('percentile_approx(best_score, 0.5)').alias('p50'),\n",
    "        F.expr('percentile_approx(best_score, 0.9)').alias('p90'),\n",
    "        F.max('best_score').alias('max_score'),\n",
    "        F.count('*').alias('pages_scored')\n",
    "    )\n",
    "    .orderBy('target_label')\n",
    ")\n",
    "\n",
    "display(score_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f2841d3-eba4-46c2-a8a0-c1cc00c07e5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print('docs rows:', docs.count())\n",
    "print('candidate rows (tok):', tok.count())\n",
    "print('pages with candidates:', tok.select('target_uri').distinct().count())\n",
    "print('pages after scoring:', scored.select('target_uri').distinct().count())\n",
    "\n",
    "display(\n",
    "    scored.orderBy(F.desc('best_score'))\n",
    "           .select('target_label','best_score','best_phrase','base_domain','target_uri')\n",
    "           .limit(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ead2c56-045b-4ad5-a39e-93c45e9e74d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sampling for manual inspection\n",
    "top_hits = (\n",
    "    scored_flagged\n",
    "    .filter(F.col('is_match'))\n",
    "    .select('target_label','best_score','best_phrase','base_domain','target_uri')\n",
    "    .orderBy(F.desc('best_score'))\n",
    "    .limit(100)\n",
    ")\n",
    "display(top_hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a0fdebd-1971-45e5-b72e-f579bf972c07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87fa92bb-0a00-4acf-914f-bf005c09067b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## --- 1) Lowercase + create text_norm (you can add more cleaning later if you want) ---\n",
    "#df = (\n",
    "#    df.withColumn(\"body\", F.lower(F.col(\"body\")))\n",
    "#      .withColumn(\"text_norm\", F.col(\"body\"))   # ensure it exists for regex below\n",
    "#)\n",
    "#\n",
    "## --- 2) Regex features + weak labels ---\n",
    "#acs_phrase_rx    = r\"(american community survey|acs(?![a-z0-9]))\"\n",
    "#census_phrase_rx = r\"(u\\.?s\\.?\\s*census\\s*bureau|united states census bureau|us census bureau)\"\n",
    "#acs_year_rx      = r\"(acs\\s*1[- ]?year|acs\\s*5[- ]?year|acs\\s+\\d{4})\"\n",
    "#table_id_rx      = r\"\\b(?:b|c|s|dp)\\d{3,6}\\b\"      # B01003, C17002, S1901, DP03...\n",
    "#source_line_rx   = r\"(source\\s*:\\s*(?:u\\.?s\\.?\\s*)?census\\s*bureau.*acs)\"\n",
    "#\n",
    "#df = (\n",
    "#    df\n",
    "#    .withColumn(\"has_acs_phrase\",    F.col(\"text_norm\").rlike(acs_phrase_rx))\n",
    "#    .withColumn(\"has_census_bureau\", F.col(\"text_norm\").rlike(census_phrase_rx))\n",
    "#    .withColumn(\"has_acs_year\",      F.col(\"text_norm\").rlike(acs_year_rx))\n",
    "#    .withColumn(\"has_table_id\",      F.col(\"text_norm\").rlike(table_id_rx))\n",
    "#    .withColumn(\"has_source_line\",   F.col(\"text_norm\").rlike(source_line_rx))\n",
    "#    .withColumn(\n",
    "#        \"weak_label\",\n",
    "#        F.when(F.col(\"has_source_line\") | (F.col(\"has_census_bureau\") & F.col(\"has_acs_phrase\")), 1)\n",
    "#         .when(F.col(\"has_table_id\") & ~F.col(\"has_census_bureau\") & ~F.col(\"has_source_line\"), 0)\n",
    "#         .otherwise(F.lit(None))\n",
    "#    )\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd37f5cc-c2b3-4238-bc69-a0bb94016d7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## --- 3) Fuzzy score (optional; only computed on likely ACS pages) ---\n",
    "#@F.udf(T.IntegerType())\n",
    "#def acs_fuzzy_score(t):\n",
    "#    if t is None:\n",
    "#        return None\n",
    "#    t = t[:5000]\n",
    "#    return int(max(fuzz.partial_ratio(t, pat) for pat in [\n",
    "#        \"source: u.s. census bureau, american community survey\",\n",
    "#        \"american community survey (acs) 5-year estimates\",\n",
    "#        \"u.s. census bureau acs table\",\n",
    "#    ]))\n",
    "#\n",
    "#df = df.withColumn(\n",
    "#    \"acs_fuzzy\",\n",
    "#    F.when(\n",
    "#        F.col(\"has_acs_phrase\") | F.col(\"has_census_bureau\") | F.col(\"has_table_id\"),\n",
    "#        acs_fuzzy_score(F.col(\"text_norm\"))\n",
    "#    )\n",
    "#).limit(10)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "883479fc-6577-4a9b-9ab2-3e2655894a08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Running time approximately `29 sec\n",
    "#\n",
    "## --- 4) Build ML set safely (no empty docs/vocab) ---\n",
    "#data = (\n",
    "#    df.filter(F.length(\"text_norm\") > 100)\n",
    "#      .withColumn(\"f_acs_fuzzy\",        F.coalesce(F.col(\"acs_fuzzy\").cast(\"double\"), F.lit(0.0)))\n",
    "#      .withColumn(\"f_has_acs_phrase\",    F.col(\"has_acs_phrase\").cast(\"double\"))\n",
    "#      .withColumn(\"f_has_census_bureau\", F.col(\"has_census_bureau\").cast(\"double\"))\n",
    "#      .withColumn(\"f_has_acs_year\",      F.col(\"has_acs_year\").cast(\"double\"))\n",
    "#      .withColumn(\"f_has_table_id\",      F.col(\"has_table_id\").cast(\"double\"))\n",
    "#      .withColumn(\"f_has_source_line\",   F.col(\"has_source_line\").cast(\"double\"))\n",
    "#)\n",
    "#\n",
    "#tok  = RegexTokenizer(inputCol=\"text_norm\", outputCol=\"tok\", pattern=r\"\\W+\")\n",
    "#stop = StopWordsRemover(inputCol=\"tok\", outputCol=\"tok_sw\")\n",
    "#tmp  = Pipeline(stages=[tok, stop]).fit(data).transform(data)\n",
    "#tmp  = tmp.filter(F.size(\"tok_sw\") > 0)   # avoid empty-token rows\n",
    "#\n",
    "## Labeled set (fallback to guarantee at least some labels)\n",
    "#labeled = tmp.filter(F.col(\"weak_label\").isNotNull())\n",
    "#if labeled.limit(1).count() == 0:\n",
    "#    labeled = tmp.withColumn(\n",
    "#        \"label\",\n",
    "#        F.when(F.col(\"has_source_line\") | (F.col(\"has_census_bureau\") & F.col(\"has_acs_phrase\")), 1).otherwise(0)\n",
    "#    )\n",
    "#else:\n",
    "#    labeled = labeled.withColumnRenamed(\"weak_label\", \"label\")\n",
    "#\n",
    "#ng2   = NGram(n=2, inputCol=\"tok_sw\", outputCol=\"tok2\")\n",
    "#\n",
    "# Text features (relaxed thresholds avoid empty vocabulary)\n",
    "#htf1  = HashingTF(inputCol=\"tok_sw\", outputCol=\"tf1\", numFeatures=1<<16)  # 65,536\n",
    "#htf2  = HashingTF(inputCol=\"tok2\",   outputCol=\"tf2\", numFeatures=1<<16)  # 65,536\n",
    "#\n",
    "#idf1  = IDF(inputCol=\"tf1\", outputCol=\"tfidf1\")\n",
    "#idf2  = IDF(inputCol=\"tf2\", outputCol=\"tfidf2\")\n",
    "#\n",
    "#asm = VectorAssembler(\n",
    "#    inputCols=[\"tfidf1\",\"tfidf2\",\"f_has_acs_phrase\",\"f_has_census_bureau\",\n",
    "#               \"f_has_acs_year\",\"f_has_table_id\",\"f_has_source_line\",\"f_acs_fuzzy\"],\n",
    "#    outputCol=\"features\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30a6a849-5fa4-4749-8cb0-1e17fb39e1de",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"target_uri\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1760223057113}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##This is taking approximately ~10 mins to run. It builds individual predictions by Website.\n",
    "#\n",
    "#lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=50)\n",
    "#\n",
    "#pipe = Pipeline(stages=[ng2, htf1, htf2, idf1, idf2, asm, lr])\n",
    "#model = pipe.fit(labeled)\n",
    "#pred  = model.transform(tmp).select(\"target_uri\",\"prediction\",\"probability\")\n",
    "#display(pred)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ML Modeling - test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
