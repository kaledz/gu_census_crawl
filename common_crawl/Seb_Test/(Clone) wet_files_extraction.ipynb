{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c39acd96-cbbf-497f-ae6d-fa327d3e7987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Wet File Data Extraction**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c15fd1f0-5a1a-4a07-b6f9-829152f1a14e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4364055b-b476-4a8b-88a0-04cc0de79a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install langid==1.1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85bee9fd-c83f-44e6-bffd-c896c4d2d1c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import boto3\n",
    "import botocore\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52b43ba1-7453-499a-8eda-83194ca68631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Connect to Boto3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94f77efa-cc51-46b5-98ba-9be74c96e12a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.1 Set Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71753b31-0137-4637-9b03-fd4591ac43b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "aws_access_key_id = dbutils.secrets.get(scope='aws_cc', key='aws_access_key_id')\n",
    "aws_secret_access_key = dbutils.secrets.get(scope='aws_cc', key='aws_secret_access_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6c9b7b0-70b1-44df-9b82-91854363519d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.2 Intialize boto3 client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90566ef5-b594-4c43-887e-d70e7dcb3560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optional: build client once (faster)\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    region_name='us-east-1',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf019db5-cdea-4a3f-b5d4-9de74194a985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.3. Text transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c82ec939-e50f-4161-8fb9-e3acdcee55c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.text(\n",
    "    's3://mydbxbucketpractice/common_crawl/wet_files/CC-MAIN-20220629054527-20220629084527-00796.warc.wet.gz'\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cb0abe1-15ea-4e6d-b0e3-042e89a132ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# 1) Add line_id column to preserve line order\n",
    "df_with_id = df.withColumn(\n",
    "    \"line_id\",\n",
    "    F.row_number().over(Window.orderBy(F.monotonically_increasing_id()))\n",
    ")\n",
    "\n",
    "# 2) Add is_start column\n",
    "df_with_id = df_with_id.withColumn(\n",
    "    'is_start',\n",
    "    F.when(F.col('value').rlike(r\"^WARC/\\d+\\.\\d+\"), F.lit(1)).otherwise(F.lit(0))\n",
    ")\n",
    "\n",
    "# 3) Cumulative sum to create record_id (ordered by line_id)\n",
    "w = Window.orderBy('line_id').rowsBetween(Window.unboundedPreceding, 0)\n",
    "df_with_id = df_with_id.withColumn('record_id', F.sum('is_start').over(w))\n",
    "\n",
    "# 4) Group lines back into records, but SORT lines by line_id inside the group\n",
    "records = (\n",
    "    df_with_id.groupBy('record_id')\n",
    "      .agg(\n",
    "          F.expr(\"\"\"\n",
    "              concat_ws(\n",
    "                '\\n',\n",
    "                transform(\n",
    "                  array_sort(collect_list(named_struct('line_id', line_id, 'value', value))),\n",
    "                  x -> x.value\n",
    "                )\n",
    "              )\n",
    "          \"\"\").alias('record_text')\n",
    "      )\n",
    "      .filter(F.col('record_text').rlike(r'^WARC/\\d+\\.\\d+'))\n",
    ")\n",
    "\n",
    "# 5) Normalize newlines to '\\n' so splitting works for both \\n\\n and \\r\\n\\r\\n\n",
    "rec2 = records.withColumn(\n",
    "    'record_text_norm',\n",
    "    F.regexp_replace('record_text', r'\\r\\n', '\\n')\n",
    ")\n",
    "\n",
    "# 6) Find separator between WARC headers and the rest (first blank line)\n",
    "rec2 = rec2.withColumn('nn_idx', F.instr('record_text_norm', '\\n\\n'))\n",
    "\n",
    "# 7) Extract headers and body using correct substring lengths\n",
    "rec2 = (\n",
    "    rec2\n",
    "    .withColumn(\n",
    "        'headers',\n",
    "        F.when(\n",
    "            F.col('nn_idx') > 0,\n",
    "            F.expr('substring(record_text_norm, 1, nn_idx - 1)')\n",
    "        ).otherwise(F.col('record_text_norm'))\n",
    "    )\n",
    "    .withColumn(\n",
    "        'body',\n",
    "        F.when(\n",
    "            F.col('nn_idx') > 0,\n",
    "            F.expr(\"\"\"\n",
    "                substring(\n",
    "                    record_text_norm,\n",
    "                    nn_idx + 2,\n",
    "                    length(record_text_norm) - (nn_idx + 1)\n",
    "                )\n",
    "            \"\"\")\n",
    "        ).otherwise(F.lit(\"\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# 8) Pull out common header fields\n",
    "parsed = (\n",
    "    rec2\n",
    "    .withColumn('warc_version',    F.regexp_extract('record_text_norm', r\"^(WARC/\\d+\\.\\d+)\", 1))\n",
    "    .withColumn('warc_type',       F.regexp_extract('headers', r\"(?m)^WARC-Type:\\s*(.*)$\", 1))\n",
    "    .withColumn('target_uri',      F.regexp_extract('headers', r\"(?m)^WARC-Target-URI:\\s*(.*)$\", 1))\n",
    "    .withColumn('warc_date',       F.regexp_extract('headers', r\"(?m)^WARC-Date:\\s*(.*)$\", 1))\n",
    "    .withColumn('warc_record_id',  F.regexp_extract('headers', r\"(?m)^WARC-Record-ID:\\s*<?(.*)>?$\", 1))\n",
    "    .withColumn('warc_refers_to',  F.regexp_extract('headers', r\"(?m)^WARC-Refers-To:\\s*<?(.*)>?$\", 1))\n",
    "    .withColumn('block_digest',    F.regexp_extract('headers', r\"(?m)^WARC-Block-Digest:\\s*(.*)$\", 1))\n",
    "    .withColumn('language',        F.regexp_extract('headers', r\"(?m)^WARC-Identified-Content-Language:\\s*(.*)$\", 1))\n",
    "    .withColumn('content_type',    F.regexp_extract('headers', r\"(?m)^Content-Type:\\s*(.*)$\", 1))\n",
    "    .withColumn(\n",
    "        'content_length',\n",
    "        F.expr(\"\"\"\n",
    "            try_cast(\n",
    "              nullif(\n",
    "                regexp_extract(headers, '(?m)^Content-Length:\\\\s*(\\\\d+)$', 1),\n",
    "                ''\n",
    "              ) as int\n",
    "            )\n",
    "        \"\"\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 9) Proper dataframe\n",
    "warc_df = parsed.select(\n",
    "    \"record_id\",\"warc_version\",\"warc_type\",\"target_uri\",\"warc_date\",\n",
    "    \"warc_record_id\",\"warc_refers_to\",\"block_digest\",\"language\",\n",
    "    \"content_type\",\"content_length\",\"body\"\n",
    ")\n",
    "\n",
    "display(warc_df.limit(5))\n",
    "print(\"total records:\", warc_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5a8376d-0113-4f36-9878-08cf97135a7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# extracting Host from URL\n",
    "df_us = (\n",
    "    warc_df\n",
    "    .withColumn(\"host\", F.expr(\"parse_url(target_uri, 'HOST')\"))   # full host (e.g., www.census.gov)\n",
    "    .filter(F.col(\"host\").isNotNull())\n",
    "    .withColumn(\"host_lc\", F.lower(\"host\"))\n",
    "    .withColumn(\"labels\", F.split(F.col(\"host_lc\"), r\"\\.\"))        # split on literal dot\n",
    "    .filter(F.size(F.col(\"labels\")) >= 2)\n",
    "    .withColumn(\"tld\", F.element_at(F.col(\"labels\"), -1))\n",
    "    .withColumn(\"sld\", F.element_at(F.col(\"labels\"), -2))\n",
    "    .withColumn(\"base_domain\", F.concat_ws(\".\", F.col(\"sld\"), F.col(\"tld\")))  # e.g., census.gov\n",
    "    .filter(F.col(\"tld\").isin(\"com\", \"org\", \"gov\", \"edu\", \"info\"))\n",
    ")\n",
    "\n",
    "display(df_us.select(\"target_uri\",\"host\",\"base_domain\").limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a652b426-64f3-4d41-aa63-7cce6bd7dcb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import langid\n",
    "\n",
    "# Helper function for detection\n",
    "def detect_text_language(text: str):\n",
    "    if isinstance(text, str) and len(text.strip()) > 20:\n",
    "        return langid.classify(text)[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# mapInPandas function\n",
    "def detect_lang(batch_iter):\n",
    "    for pdf in batch_iter:\n",
    "        pdf[\"lang\"] = pdf[\"body\"].apply(detect_text_language)\n",
    "        yield pdf\n",
    "\n",
    "# Apply to dataframe\n",
    "warc_with_lang = df_us.mapInPandas(\n",
    "    detect_lang, \n",
    "    schema=df_us.schema.add(\"lang\", \"string\")\n",
    ")\n",
    "\n",
    "# Keep only English\n",
    "warc_en = warc_with_lang.filter(F.col(\"lang\") == \"en\")\n",
    "\n",
    "display(warc_en.limit(100))\n",
    "print(\"English docs:\", warc_en.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "966afac3-f105-48a5-a2aa-6f91b55b3f1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f28d160e-1b05-4a95-b1d8-2cf2a4fed9b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "def normalize_text(col=\"body\"):\n",
    "    return F.trim(F.regexp_replace(F.lower(F.col(col)), r\"\\s+\", \" \"))\n",
    "\n",
    "docs = (\n",
    "    warc_en\n",
    "    .select(\"target_uri\", \"body\", \"host\", \"base_domain\")\n",
    "    .withColumn(\"text_norm\", normalize_text(\"body\"))\n",
    "    .filter(F.length(\"text_norm\") > 100)\n",
    ")\n",
    "\n",
    "\n",
    "# 3. Apply your regex dictionary keys\n",
    "for key, pattern in acs_terms_2.items():\n",
    "    docs = docs.withColumn(key, F.col(\"text_norm\").rlike(pattern))\n",
    "\n",
    "\n",
    "# 4. Derive higher-level ACS signal flags\n",
    "docs = (\n",
    "    docs\n",
    "    .withColumn(\n",
    "        \"has_any_acs\",\n",
    "        F.col(\"acs_phrase\") |\n",
    "        F.col(\"acs_detail_tables\") |\n",
    "        F.col(\"acs_subject_tables\") |\n",
    "        F.col(\"acs_table_word\") |\n",
    "        F.col(\"acs_geo\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"has_citation_phrase\",\n",
    "        F.col(\"acs_links\") | F.col(\"acs_near_table\") | F.col(\"acs_near_moe\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1da47b76-d2db-4551-92ec-0e36e0da23a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "label_udf = F.udf(\n",
    "    lambda a, c: (\n",
    "        \"cites_census\" if c and a else\n",
    "        (\"repackages_census\" if a and not c else \"unrelated\")\n",
    "    ),\n",
    "    T.StringType()\n",
    ")\n",
    "\n",
    "labeled_pages = docs.withColumn(\n",
    "    \"page_label\",\n",
    "    label_udf(F.col(\"has_any_acs\"), F.col(\"has_citation_phrase\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31f686fe-a199-4573-8b0b-c35e05a7683b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install thefuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "835b5e89-3b2a-4be3-9eea-c3a35e383030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "site_rollup = (\n",
    "    labeled_pages\n",
    "    .groupBy(\"base_domain\")\n",
    "    .agg(\n",
    "        F.sum(F.when(F.col(\"page_label\") == \"cites_census\", 1).otherwise(0)).alias(\"pages_cite\"),\n",
    "        F.sum(F.when(F.col(\"page_label\") == \"repackages_census\", 1).otherwise(0)).alias(\"pages_repackage\"),\n",
    "        F.sum(F.when(F.col(\"page_label\") != \"unrelated\", 1).otherwise(0)).alias(\"pages_about\"),\n",
    "        F.count(\"*\").alias(\"pages_total\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"pct_cite\",\n",
    "        F.when(F.col(\"pages_about\") > 0, F.col(\"pages_cite\") / F.col(\"pages_about\")).otherwise(F.lit(0.0))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"pct_repackage\",\n",
    "        F.when(F.col(\"pages_about\") > 0, F.col(\"pages_repackage\") / F.col(\"pages_about\")).otherwise(F.lit(0.0))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"site_label\",\n",
    "        F.when(F.col(\"pct_cite\") >= 0.5, \"cites_census\")\n",
    "         .when(F.col(\"pct_repackage\") >= 0.5, \"repackages_census\")\n",
    "         .otherwise(\"mixed_or_unclear\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f922fe6-a118-4c88-b697-d51a551cf4c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## This block takes approximately ~7 mins\n",
    "\n",
    "# Spark ML + MLflow imports\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, SQLTransformer, NGram, CountVectorizer, IDF, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import functions as F\n",
    "import mlflow\n",
    "\n",
    "# 0) Disable MLflow autologging on serverless + Connect \n",
    "try:\n",
    "    mlflow.autolog(disable=True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 1) Data\n",
    "data = (\n",
    "    labeled_pages\n",
    "    .select(\"text_norm\", \"page_label\", \"base_domain\")\n",
    "    .filter(F.col(\"text_norm\").isNotNull() & (F.length(\"text_norm\") > 100))\n",
    "    .filter(F.col(\"page_label\").isNotNull())\n",
    ")\n",
    "\n",
    "data = data.withColumn(\n",
    "    \"text_norm\",\n",
    "    F.lower(F.col(\"text_norm\"))\n",
    ")\n",
    "\n",
    "display(data.groupBy(\"page_label\").count().orderBy(F.desc(\"count\")))\n",
    "print(\"Training rows:\", data.count())\n",
    "\n",
    "train, test = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 2) Featurization\n",
    "tokenizer = RegexTokenizer(\n",
    "    inputCol=\"text_norm\",\n",
    "    outputCol=\"tokens\",\n",
    "    pattern=r\"\\W+\"\n",
    ")\n",
    "stopper   = StopWordsRemover(\n",
    "    inputCol=\"tokens\", \n",
    "    outputCol=\"tokens_nostop\")\n",
    "\n",
    "bigrams   = NGram(\n",
    "    n=2, \n",
    "    inputCol=\"tokens_nostop\", \n",
    "    outputCol=\"bigrams\")\n",
    "\n",
    "merge_tokens = SQLTransformer(\n",
    "    statement=\"SELECT *, array_concat(tokens_nostop, bigrams) AS toks FROM __THIS__\")\n",
    "\n",
    "\n",
    "cv = CountVectorizer(\n",
    "    inputCol=\"toks\", \n",
    "    outputCol=\"tf\",\n",
    "    vocabSize=30000, \n",
    "    minDF=10, \n",
    "    binary=True)\n",
    "\n",
    "assembler_tf = VectorAssembler(\n",
    "    inputCols=[\"tf_uni\", \"tf_bi\"], \n",
    "    outputCol=\"tf_all\")\n",
    "\n",
    "idf   = IDF(inputCol=\"tf\", \n",
    "            outputCol=\"tfidf\", \n",
    "            minDocFreq=5)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) wet_files_extraction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
