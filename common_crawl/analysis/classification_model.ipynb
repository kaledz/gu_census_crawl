{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b465ebd-1889-49d9-934e-b89838bae784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import math\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    RegexTokenizer,\n",
    "    StopWordsRemover,\n",
    "    NGram,\n",
    "    CountVectorizer,\n",
    "    IDF,\n",
    "    VectorAssembler,\n",
    "    StringIndexer,\n",
    "    HashingTF,\n",
    "    IndexToString,\n",
    "    ChiSqSelector\n",
    ")\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression,\n",
    "    LinearSVC,\n",
    "    RandomForestClassifier,\n",
    "    OneVsRest\n",
    ")\n",
    "from pyspark.ml.evaluation import (\n",
    "    BinaryClassificationEvaluator,\n",
    "    MulticlassClassificationEvaluator\n",
    ")\n",
    "from pyspark.ml.linalg import (\n",
    "    SparseVector,\n",
    "    DenseVector,\n",
    "    Vectors,\n",
    "    Vector\n",
    ")\n",
    "from pyspark.ml.tuning import (\n",
    "    CrossValidator,\n",
    "    ParamGridBuilder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7de87312-3605-4bb5-9d8f-e8cc5ebaa7da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"census_bureau_capstone.silver.census_product_cleaned\")\n",
    "\n",
    "print(df.count())\n",
    "display(df.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0323a34d-7223-44ca-bf09-6b9799822cc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def normalize_text(colname=\"text\"):\n",
    "    return F.trim(F.regexp_replace(F.lower(F.col(colname)), r\"\\s+\", \" \"))\n",
    "\n",
    "df = df.withColumn(\"text_norm\", normalize_text(\"content\"))\n",
    "df = df.filter(F.length(\"text_norm\") > 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eac931d-3e74-425c-94ea-66ab31f32c01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Core Bureau names / general references ---\n",
    "census_terms = {\n",
    "    # Bureau & generic references\n",
    "    \"census_bureau\":         r\"\\b(u\\.?\\s*s\\.?\\s*)?census\\s*bureau\\b\",\n",
    "    \"us_census\":             r\"\\bu\\.?\\s*s\\.?\\s*census\\b\",\n",
    "    \"census_generic\":        r\"\\b(the\\s+)?census(?!\\s*of\\s*agriculture)\\b\",  # avoid agriculture (USDA)\n",
    "    \"census_gov_domain\":     r\"\\b(census\\.gov|data\\.census\\.gov|api\\.census\\.gov|factfinder\\.census\\.gov)\\b\",\n",
    "\n",
    "    # Decennial Census (Population & Housing)\n",
    "    \"decennial_census\":      r\"\\b(decennial\\s+census|census\\s+20\\d{2}|population\\s+and\\s+housing\\s+census)\\b\",\n",
    "    \"sf_summary_files\":      r\"\\b(summary\\s+file\\s*(?:1|2)|sf(?:1|2)\\b)\",\n",
    "    \"redistricting_pl94\":    r\"\\b(p\\.?l\\.?\\s*94[-–]171|redistricting\\s+data)\\b\",\n",
    "    \"pums\":                  r\"\\b(public\\s+use\\s+microdata\\s+sample|pums)\\b\",\n",
    "    \"short_long_form\":       r\"\\b(short\\s+form|long\\s+form)\\b\",\n",
    "\n",
    "    # Economic programs & products\n",
    "    \"economic_census\":       r\"\\beconomic\\s+census\\b\",\n",
    "    \"census_of_govts\":       r\"\\bcensus\\s+of\\s+governments\\b\",\n",
    "    \"cbp\":                   r\"\\b(county\\s+business\\s+patterns|cbp)\\b\",\n",
    "    \"bds\":                   r\"\\b(business\\s+dynamics\\s+statistics|bds)\\b\",\n",
    "    \"lbd\":                   r\"\\b(longitudinal\\s+business\\s+database|lbd)\\b\",\n",
    "    \"qwi_led_onthemap\":      r\"\\b(qwi|lehd|lodes|on\\s*the\\s*map|onthemap)\\b\",\n",
    "\n",
    "    # Household surveys managed by Census (select)\n",
    "    \"cps\":                   r\"\\b(current\\s+population\\s+survey|cps)\\b\",\n",
    "    \"sipp\":                  r\"\\b(survey\\s+of\\s+income\\s+and\\s+program\\s+participation|sipp)\\b\",\n",
    "    \"ahs\":                   r\"\\b(american\\s+housing\\s+survey|ahs)\\b\",\n",
    "    # note: keep ACS separate; included later but your ask is “beyond ACS”\n",
    "\n",
    "    # Geography / reference files\n",
    "    \"tiger_line\":            r\"\\b(tiger(?:/line)?\\s*(?:shapefiles?)?|tiger/line|tigerline)\\b\",\n",
    "    \"gazetteer\":             r\"\\bgazetteer\\b\",\n",
    "    \"fips\":                  r\"\\bfips\\s*(?:codes?)?\\b\",\n",
    "    \"tract_block_group\":     r\"\\b(census\\s*tracts?|block\\s*groups?|census\\s*blocks?)\\b\",\n",
    "    \"places_counties_mcd\":   r\"\\b(incorporated\\s*places?|counties|minor\\s*civil\\s*divisions?|mcds?)\\b\",\n",
    "    \"puma\":                  r\"\\b(puma|public\\s+use\\s+microdata\\s+area[s]?)\\b\",\n",
    "\n",
    "    # Data portals / APIs / legacy brands\n",
    "    \"data_portals\":          r\"\\b(data\\.census\\.gov|api\\.census\\.gov|factfinder|american\\s+factfinder)\\b\",\n",
    "    \"microdata_api\":         r\"\\b(microdata\\s*api|pums\\s*api)\\b\",\n",
    "\n",
    "    # Attribution lines and “according to” language\n",
    "    \"attribution_1\":         r\"(source\\s*:\\s*(?:the\\s+)?u\\.?\\s*s\\.?\\s*census\\s*bureau)\",\n",
    "    \"attribution_2\":         r\"(according\\s+to\\s+(?:the\\s+)?u\\.?\\s*s\\.?\\s*census\\s*bureau)\",\n",
    "    \"attribution_3\":         r\"(from\\s+(?:the\\s+)?u\\.?\\s*s\\.?\\s*census\\s*bureau)\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e7b985d-290b-4663-a1d5-77f5b3cf1545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add a boolean column per pattern\n",
    "for colname, pattern in census_terms.items():\n",
    "    df = df.withColumn(colname, F.col(\"text_norm\").rlike(pattern))\n",
    "\n",
    "# Signal columns for census term detection\n",
    "signal_cols = list(census_terms.keys())\n",
    "df = df.withColumn(\n",
    "    \"has_any_census\",\n",
    "    F.array_max(F.array(*[F.col(c).cast(\"int\") for c in signal_cols])) == 1\n",
    ")\n",
    "\n",
    "# Numeric density feature\n",
    "digits_len = F.length(F.regexp_replace(F.col(\"text_norm\"), r\"[^0-9]\", \"\"))\n",
    "text_len = F.length(F.col(\"text_norm\"))\n",
    "df = df.withColumn(\n",
    "    \"num_density\",\n",
    "    digits_len / F.when(text_len > 0, text_len).otherwise(F.lit(1.0))\n",
    ")\n",
    "\n",
    "# Attribution and domain signals\n",
    "df = (\n",
    "    df.withColumn(\n",
    "        \"has_explicit_attrib\",\n",
    "        F.col(\"attribution_1\") | F.col(\"attribution_2\") | F.col(\"attribution_3\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"has_census_domain\",\n",
    "        F.col(\"census_gov_domain\") | F.col(\"data_portals\") | F.col(\"microdata_api\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"repackage_heuristic\",\n",
    "        (F.col(\"num_density\") > 0.12)\n",
    "        & (~F.col(\"has_explicit_attrib\"))\n",
    "        & (~F.col(\"has_census_domain\"))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a59f268b-5ef2-4377-9bae-f18618246992",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define label signals\n",
    "cites_signal = (\n",
    "    F.col(\"has_explicit_attrib\") | \n",
    "    F.col(\"has_census_domain\")\n",
    ")\n",
    "\n",
    "repack_signal = (\n",
    "    (F.col(\"num_density\") > 0.12) & \n",
    "    (~F.col(\"has_explicit_attrib\")) & \n",
    "    (~F.col(\"has_census_domain\"))\n",
    ")\n",
    "\n",
    "# Assign labels\n",
    "df_lab = (\n",
    "    df.withColumn(\n",
    "        \"label\",\n",
    "        F.when(cites_signal, \"cites\")\n",
    "         .when(repack_signal, \"repackages\")\n",
    "    )\n",
    "    .filter(F.col(\"label\").isNotNull())\n",
    ")\n",
    "\n",
    "# Save and display label counts\n",
    "df_lab.write.mode(\"overwrite\").saveAsTable(\"census_bureau_capstone.gold.census_repackaged_enriched\")\n",
    "print(df_lab.count())\n",
    "print(\"Label counts:\")\n",
    "display(df_lab.groupBy(\"label\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5cf0b63-37cc-4ed4-bc59-c1b7e48a8449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Graphing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c311dbf6-2004-4ac8-b74d-607d081944e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "label_counts_pdf = (\n",
    "    df_lab.groupBy(\"label\")\n",
    "          .count()\n",
    "          .orderBy(\"label\")\n",
    "          .toPandas()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(label_counts_pdf[\"label\"], label_counts_pdf[\"count\"])\n",
    "plt.title(\"Appendix A: Label distribution\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ecc3403-101b-4b36-a1b0-bbbdb9ed5851",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"Target-URI\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1763437014999}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_f2ec5fd0\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_301c494\",\"enabled\":true,\"columnId\":\"label\",\"dataType\":\"string\",\"filterType\":\"oneof\"}],\"local\":false,\"updatedAt\":1763437003744}],\"syncTimestamp\":1763437003749}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_lab.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2157843-9f03-4589-afab-b9d74893b69b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "label_counts = (\n",
    "    df_lab.groupBy(\"label\")\n",
    "    .count()\n",
    "    .collect()\n",
    ")\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1ec0bad-8bc8-4c49-9b4d-12da190e578f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compute label counts dictionary\n",
    "counts = {row[\"label\"]: row[\"count\"] for row in label_counts}\n",
    "\n",
    "# Find the maximum count for normalization\n",
    "max_count = max(counts.values())\n",
    "\n",
    "# Assign higher weight to rare classes\n",
    "class_weights = {\n",
    "    label: float(max_count) / count\n",
    "    for label, count in counts.items()\n",
    "}\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54509b77-fc2e-4fd8-8764-dbd4c4bbbfd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build class_weight column using class_weights dictionary\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "weight_expr = None\n",
    "for lbl, w in class_weights.items():\n",
    "    cond = F.col(\"label\") == lbl\n",
    "    if weight_expr is None:\n",
    "        weight_expr = F.when(cond, w)\n",
    "    else:\n",
    "        weight_expr = weight_expr.when(cond, w)\n",
    "weight_expr = weight_expr.otherwise(1.0)\n",
    "\n",
    "df_w = df_lab.withColumn(\"class_weight\", weight_expr)\n",
    "display(df_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07e902b5-3651-4f56-977c-b40bc581cd64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1) Label indexer: Converts string labels (\"cites\", \"repackages\") to numeric indices (0/1)\n",
    "label_indexer = StringIndexer(\n",
    "    inputCol=\"label\",\n",
    "    outputCol=\"label_idx\"\n",
    ")\n",
    "\n",
    "# 2) Text pipeline: Tokenizes and cleans text for feature extraction\n",
    "tokenizer = RegexTokenizer(\n",
    "    inputCol=\"text_norm\",      # normalized text column\n",
    "    outputCol=\"tokens\",        # output token list\n",
    "    pattern=\"\\\\W\"              # split on non-word characters\n",
    ")\n",
    "\n",
    "remover = StopWordsRemover(\n",
    "    inputCol=\"tokens\",         # input tokens\n",
    "    outputCol=\"filtered\"       # output tokens with stopwords removed\n",
    ")\n",
    "\n",
    "hashing = HashingTF(\n",
    "    inputCol=\"filtered\",       # input filtered tokens\n",
    "    outputCol=\"tf_raw\",        # output term frequency vector\n",
    "    numFeatures=2**12          # number of features for hashing\n",
    ")\n",
    "\n",
    "# 3) Heuristic features: Ensure numeric features are double type for ML compatibility\n",
    "heuristic_cols = [\n",
    "    \"num_density\",             # numeric density of digits in text\n",
    "    \"has_any_census\",          # boolean signal for census term detection\n",
    "]\n",
    "\n",
    "for colname in heuristic_cols:\n",
    "    df_w = df_w.withColumn(colname, F.col(colname).cast(\"double\"))\n",
    "\n",
    "# 4) Assemble all features: Combines text features and heuristics into a single feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"tf_raw\"],      # currently only text features; add heuristics if needed\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddcdb76f-5ec9-4a43-917c-bf0cdefc22f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SVM Model - Might be too heavy to run on the free version of Databrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66b3e8d7-1128-41e0-bc60-642b24d93ca8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5) SVM model\n",
    "svm = LinearSVC(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label_idx\",\n",
    "    weightCol=\"class_weight\",    # handles imbalance\n",
    "    maxIter=50,\n",
    "    regParam=0.1\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    label_indexer,\n",
    "    tokenizer,\n",
    "    remover,\n",
    "    hashing,\n",
    "    assembler,\n",
    "    svm\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e9386c5-a62a-4938-a0de-cfb1c570fb4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = df_w.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "svm_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faea3952-b223-4413-b815-bf5ad5d21da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "pred = svm_model.transform(test_df)\n",
    "\n",
    "# Evaluate AUC\n",
    "auc_eval = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label_idx\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "auc = auc_eval.evaluate(pred)\n",
    "\n",
    "# Evaluate F1 score\n",
    "f1_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_idx\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "f1 = f1_eval.evaluate(pred)\n",
    "\n",
    "print(\"AUC:\", auc)\n",
    "print(\"F1:\", f1)\n",
    "\n",
    "# Confusion matrix by original string label\n",
    "display(\n",
    "    pred.groupBy(\"label\", \"prediction\")\n",
    "        .count()\n",
    "        .orderBy(\"label\", \"prediction\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1ca2f94-243f-40e6-9bb4-4e664664c81a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# True Positives (TP) and False Negatives (FN) for \"repackages\"\n",
    "pred_repack = pred.filter(F.col(\"label\") == \"repackages\")\n",
    "tp = pred_repack.filter(F.col(\"prediction\") == 1).count()\n",
    "fn = pred_repack.filter(F.col(\"prediction\") == 0).count()\n",
    "\n",
    "# True Negatives (TN) and False Positives (FP) for \"cites\"\n",
    "pred_cites = pred.filter(F.col(\"label\") == \"cites\")\n",
    "tn = pred_cites.filter(F.col(\"prediction\") == 0).count()\n",
    "fp = pred_cites.filter(F.col(\"prediction\") == 1).count()\n",
    "\n",
    "# Precision and recall for \"repackages\"\n",
    "precision_repack = tp / float(tp + fp) if (tp + fp) > 0 else None\n",
    "recall_repack = tp / float(tp + fn) if (tp + fn) > 0 else None\n",
    "\n",
    "print(\"Repackages precision:\", precision_repack)\n",
    "print(\"Repackages recall   :\", recall_repack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b420c5e-3a7f-476e-a279-1a97621297d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Logistic Regression \n",
    "\n",
    "SVM is a heavy model. The more data that is ingested, the less likely it'll be able to run in the free version of Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c0ee2da-c386-4d27-b21c-f651775d3c1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label_idx\",\n",
    "    weightCol=\"class_weight\",\n",
    "    maxIter=30,\n",
    "    regParam=0.1\n",
    ")\n",
    "\n",
    "pipeline_lr = Pipeline(stages=[\n",
    "    label_indexer,\n",
    "    tokenizer,\n",
    "    remover,\n",
    "    hashing,\n",
    "    assembler,\n",
    "    lr\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5524fcc0-1de9-4511-b4d3-54ce743c4f40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr_model = pipeline_lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b624cf0c-63a9-42d5-bc4c-b150e116a259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate predictions with logistic regression\n",
    "pred_lr = lr_model.transform(test_df)\n",
    "\n",
    "# Evaluate AUC\n",
    "auc_eval_lr = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label_idx\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "auc_lr = auc_eval_lr.evaluate(pred_lr)\n",
    "\n",
    "# Evaluate F1 score\n",
    "f1_eval_lr = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_idx\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "f1_lr = f1_eval_lr.evaluate(pred_lr)\n",
    "\n",
    "print(\"AUC:\", auc_lr)\n",
    "print(\"F1:\", f1_lr)\n",
    "\n",
    "# Confusion matrix by original string label\n",
    "display(\n",
    "    pred_lr.groupBy(\"label\", \"prediction\")\n",
    "          .count()\n",
    "          .orderBy(\"label\", \"prediction\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2759fa4f-292a-468a-892e-7d16a6a2e1cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For quick sanity check specifically on \"repackages\"\n",
    "\n",
    "# True Positives (TP) and False Negatives (FN) for \"repackages\"\n",
    "pred_lr_repack = pred_lr.filter(F.col(\"label\") == \"repackages\")\n",
    "tp_lr = pred_lr_repack.filter(F.col(\"prediction\") == 1).count()\n",
    "fn_lr = pred_lr_repack.filter(F.col(\"prediction\") == 0).count()\n",
    "\n",
    "# True Negatives (TN) and False Positives (FP) for \"cites\"\n",
    "pred_lr_cites = pred_lr.filter(F.col(\"label\") == \"cites\")\n",
    "tn_lr = pred_lr_cites.filter(F.col(\"prediction\") == 0).count()\n",
    "fp_lr = pred_lr_cites.filter(F.col(\"prediction\") == 1).count()\n",
    "\n",
    "# Precision and recall for \"repackages\"\n",
    "precision_lr_repack = (\n",
    "    tp_lr / float(tp_lr + fp_lr) if (tp_lr + fp_lr) > 0 else None\n",
    ")\n",
    "recall_lr_repack = (\n",
    "    tp_lr / float(tp_lr + fn_lr) if (tp_lr + fn_lr) > 0 else None\n",
    ")\n",
    "\n",
    "print(\"Repackages precision:\", precision_lr_repack)\n",
    "print(\"Repackages recall   :\", recall_lr_repack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e2d7bfc-e80d-41d3-97f4-ad09c5daa938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3543a3b4-810f-40f8-b13c-566d1cc46ca8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cosine Similarity (CS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2935a8d-a5f9-45e0-b161-24ea7dad8ad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###### This block should be able to grab unique URLs from the dataset \n",
    "\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Define window: partition by URI, order by longest normalized text\n",
    "window = (\n",
    "    Window\n",
    "    .partitionBy(\"Target-URI\")\n",
    "    .orderBy(F.desc(F.length(\"text_norm\")))\n",
    ")\n",
    "\n",
    "# Select one row per URI (longest text), drop helper column\n",
    "df_unique = (\n",
    "    df_w\n",
    "    .withColumn(\"rn\", F.row_number().over(window))\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96d501d9-c270-4ead-8627-2526004f0fa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build text vectorization pipeline\n",
    "text_vec_pipeline = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    remover,\n",
    "    hashing\n",
    "])\n",
    "\n",
    "# Fit and transform to get vectorized features\n",
    "text_vec_model = text_vec_pipeline.fit(df_unique)\n",
    "df_vec = text_vec_model.transform(df_unique)\n",
    "\n",
    "# Rename tf_raw to features for cosine similarity\n",
    "df_vec = df_vec.withColumnRenamed(\"tf_raw\", \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ebcc084-3b6c-4107-bb0a-29017534d15c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Setting up \"anchor\" URI with clear repackaged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a5118a8-3345-4d3e-96fa-e6261351f7a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1) Already identified URI with repackaged data\n",
    "anchor_uri = \"https://www.schooldigger.com/go/IN/schools/0345000465/school.aspx\"\n",
    "\n",
    "# Find anchor row by URI\n",
    "anchor_row = (\n",
    "    df_vec\n",
    "    .filter(F.col(\"Target-URI\") == anchor_uri)\n",
    "    .select(\"Target-URI\", \"label\", \"features\")\n",
    "    .limit(1)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "if not anchor_row:\n",
    "    raise ValueError(f\"Anchor URI {anchor_uri} not found in df_vec\")\n",
    "\n",
    "anchor_vec = anchor_row[0][\"features\"]\n",
    "anchor_label = anchor_row[0][\"label\"]\n",
    "\n",
    "print(\"Anchor label:\", anchor_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a49126b0-8fdb-4b02-916d-0b5e7cf39239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Defining CS UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e8ca6ad-562c-494c-ad6c-a3aaec01ff1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cosine_sim_py(v: Vector, anchor: Vector) -> float:\n",
    "    if v is None or anchor is None:\n",
    "        return None\n",
    "    dot_product = float(v.dot(anchor))\n",
    "    norm_v = math.sqrt(float(v.dot(v)))\n",
    "    norm_anchor = math.sqrt(float(anchor.dot(anchor)))\n",
    "    if norm_v == 0.0 or norm_anchor == 0.0:\n",
    "        return None\n",
    "    return dot_product / (norm_v * norm_anchor)\n",
    "\n",
    "cosine_sim_udf = F.udf(\n",
    "    lambda v: float(cosine_sim_py(v, anchor_vec)),\n",
    "    DoubleType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d90c6d1e-6b99-426b-aee6-ad6f5854c72a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "# Extract domain from anchor URI\n",
    "domain = urlparse(anchor_uri).netloc\n",
    "\n",
    "# Compute cosine similarity and filter out same domain\n",
    "df_sim = (\n",
    "    df_vec\n",
    "    .withColumn(\"cosine_to_anchor\", cosine_sim_udf(\"features\"))\n",
    "    # Exclude rows from the same domain as anchor\n",
    "    .filter(~F.col(\"Target-URI\").contains(domain))\n",
    ")\n",
    "\n",
    "# Display top 20 most similar rows\n",
    "display(\n",
    "    df_sim\n",
    "    .select(\n",
    "        \"Target-URI\",\n",
    "        \"label\",\n",
    "        \"cosine_to_anchor\",\n",
    "        \"num_density\",\n",
    "        \"has_any_census\"\n",
    "    )\n",
    "    .orderBy(F.desc(\"cosine_to_anchor\"))\n",
    "    .limit(20)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "classification_model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
