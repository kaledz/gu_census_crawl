{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e366f98d-435c-4469-a2d8-469cef6ce3a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **S3 CC Data Ingestion**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d440be8c-b5f5-45a6-ac06-96f333171505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1: Bronze Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fef6b5e6-e876-4a90-8800-26995760ffa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1.1: import libraries\n",
    "- import cc (commona crawl) class from src folder\n",
    "- import all other required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "396b82e2-cdbf-4ab6-9e4d-dc33aca1b702",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\n",
    "    '/Workspace/Shared/gu_census_crawl/common_crawl/cc_segement_ingestion/src/cc_segement_ingestion'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed7a278d-0b5f-4b71-b159-a7e4cc8e067e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from helpers import *\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "908e9711-73f1-4584-901a-586f9b397b2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1.2: Set Secret Variables\n",
    "- Call secrets that are stored in databricks utilities for aws credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b678880-dabe-4963-9b00-44408ae06a90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "aws_access_key_id = dbutils.secrets.get(scope='aws_cc', key='aws_access_key_id')\n",
    "aws_secret_access_key = dbutils.secrets.get(scope='aws_cc', key='aws_secret_access_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44d7fa5d-e2b7-4922-99c2-d3744db58330",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optional: build client once (faster)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    region_name=\"us-east-1\",\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79db08fd-7abf-4fab-9e5c-040442529f24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the cleaned master crawls table from the silver layer into a Spark DataFrame\n",
    "df_master_crawls_2025 = spark.read.table(\"census_bureau_capstone.silver.cleaned_master_crawls_2025\")\n",
    "\n",
    "# Convert the Spark DataFrame to a Pandas DataFrame\n",
    "df_master_crawls_2025_pandas = df_master_crawls_2025.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c33d309-8308-46b5-984f-622490ed03d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1.4: Call list crawls method\n",
    "- Call the list_crawls() function from the helpers \n",
    "- utlize batching to iterate and union crawls from filtered master crawls dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f230e872-fb4b-418a-b860-6f5fca5bc762",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate a list of crawls using the batch_crawl_list method from the cc class\n",
    "crawl_list = cc.batch_crawl_list(df_master_crawls_2025_pandas, \"master_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a41caed-1884-4818-92d8-3cf84d54be36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ingest the crawls from the crawl list using the batch_ingest_crawls method\n",
    "df_crawls = cc.batch_ingest_crawls(crawl_list)\n",
    "\n",
    "# Write the ingested crawls DataFrame to the bronze layer table, overwriting any existing data\n",
    "df_crawls.write. \\\n",
    "    mode(\"overwrite\"). \\\n",
    "    saveAsTable(\"`census_bureau_capstone`.bronze.raw_all_crawls\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "batch_2025_crawls",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
